{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "---\n",
    "\n",
    "During the course of doing data analysis and modeling, a significant amount of time is spent on data preparation: loading, cleaning, transforming, and rearranging. Such tasks often takes 80% or more of an analyst’s time. Sometimes the way that data is stored in files or databases is not in the right format for a particular task. Fortunately, pandas, along with the built-in Python language features, provides you with a high-level, flexible, and fast set of tools to enable you to manipulate data into the right form.\n",
    "\n",
    "\n",
    "### Lecture outline\n",
    "\n",
    "---\n",
    "\n",
    "* Finding and Filling Missing Values\n",
    "\n",
    "\n",
    "* Removing Duplicate Values\n",
    "\n",
    "\n",
    "* Replacing Values\n",
    "\n",
    "\n",
    "* Discretization and Binning\n",
    "\n",
    "\n",
    "* Detecting Outliers\n",
    "\n",
    "\n",
    "* String Manipulations\n",
    "\n",
    "\n",
    "* Variable Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Finding and Filling Missing Values\n",
    "\n",
    "---\n",
    "\n",
    "Missing values are pretty common in data cleaning activities. And, they can be there for any number of reasons.\n",
    "\n",
    "\n",
    "For instance, if you are running a survey and a respondent didn't answer a question the missing value is\n",
    "actually an omission. This kind of missing data is called **Missing at Random** if there are other variables\n",
    "that might be used to predict the variable which is missing. If there is no relationship to other variables, then  we call this data **Missing Completely at Random (MCAR)**, in other words, missing is independent of the observed and unobserved data. **Missing not at random (MNAR)**. When data are MNAR, the fact that the data are missing is systematically related to the unobserved data, that is, the missingness is related to events or factors which are not measured by the researcher.\n",
    "\n",
    "\n",
    "Pandas works with missing data as painless as possible. For example, all of the descriptive statistics on pandas objects exclude missing data by default.\n",
    "\n",
    "\n",
    "> Not only `NaN`, `NA`, `N/A`, `NAT`, `NULL`, and `None` are missing values. There can be other missing values in the data. That's why we need always to know our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Reference\n",
    "\n",
    "[Types of Missing Data](https://www.ncbi.nlm.nih.gov/books/NBK493614/)\n",
    "\n",
    "\n",
    "[Working with missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![alt text](images/missing_data.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T09:42:47.332418Z",
     "start_time": "2021-01-01T09:42:47.051024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T09:42:47.387146Z",
     "start_time": "2021-01-01T09:42:47.350389Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"data/weather.csv\")\n",
    "\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Detecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:07:54.857962Z",
     "start_time": "2020-12-31T11:07:54.842877Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather.isnull() # Retruns boolean seris. True denotes missing value\n",
    "\n",
    "\n",
    "weather.isna() # Same as \"isnull()\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:08:21.056442Z",
     "start_time": "2020-12-31T11:08:21.042640Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather.isnull().any() # Shows all columns with missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:08:34.665116Z",
     "start_time": "2020-12-31T11:08:34.627255Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather[weather.isnull().any(axis=1)] # Show all rows with missing values\n",
    "\n",
    "# Something weird happens in \"MIN_TEMP_GROUND\" column - We'll see later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:09:00.238993Z",
     "start_time": "2020-12-31T11:09:00.235290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather.isnull().all(axis=1).any() # Are there any rows with only null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:09:25.789263Z",
     "start_time": "2020-12-31T11:09:25.784490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather.notnull().all() # Are there any columns with no null values at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:14:35.749417Z",
     "start_time": "2020-12-31T11:14:35.745860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather[\"MIN_TEMP_GROUND\"].head(30) # Do you see pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:16:02.690058Z",
     "start_time": "2020-12-31T11:16:02.678333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "every_6th_row_index = pd.Series(range(5, len(weather), 6)) # Indices for every 6th row\n",
    "\n",
    "every_6th_row_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:16:27.760641Z",
     "start_time": "2020-12-31T11:16:27.756178Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather[\"MIN_TEMP_GROUND\"][every_6th_row_index].notnull().all() # Are all these rows NOT null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:21:58.705527Z",
     "start_time": "2020-12-31T11:21:58.700699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather['MIN_TEMP_GROUND'].drop(every_6th_row_index).isnull().all() # Are all other rows null?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "---\n",
    "\n",
    "The strategy of handling missing values depends on the type of missing value and/or the problem and data at hand. We may have huge amount of data and dropping missing values will not affect our aims, or we may have small amount of data and it's desirable to impute the missing values.\n",
    "\n",
    "Let see how can we drop/remove missing values in rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:33:05.708758Z",
     "start_time": "2020-12-31T11:33:05.701549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "series = pd.Series([1, np.nan, 3.5, np.nan, 7, 10, np.nan])\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:33:46.779904Z",
     "start_time": "2020-12-31T11:33:46.776227Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "series.dropna() # Removes missing values\n",
    "\n",
    "\n",
    "series[series.notnull()] # Same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:39:46.785537Z",
     "start_time": "2020-12-31T11:39:46.745839Z"
    },
    "hidden": true
   },
   "source": [
    "Dropping missing values from DataFrame is somewhat different from dropping missing values from Series. For that reason, I create sample DataFrame, to show the effect of `dropna()` method on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:42:01.895583Z",
     "start_time": "2020-12-31T11:42:01.888865Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n",
    "                           [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]], columns=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:47:33.562011Z",
     "start_time": "2020-12-31T11:47:33.553971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df.dropna(axis=0, how=\"any\") # The shortest solution - drop everything (Generally not good idea!!!)\n",
    "\n",
    "\n",
    "missing_df.dropna(axis=0, how=\"all\") # Remove rows if all values are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:48:02.736782Z",
     "start_time": "2020-12-31T11:48:02.728719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df.dropna(axis=1, how=\"any\") # Drop column if contains at least one missing value\n",
    "\n",
    "missing_df.dropna(axis=1, how=\"all\") # Drop column if all values are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:52:13.942061Z",
     "start_time": "2020-12-31T11:52:13.935455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df.dropna(axis=0, how=\"any\", subset=[\"a\"]) # Filter out missing values by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rather than filtering out missing data (and potentially discarding other data along with it), you may want to fill in the “holes” in any number of ways. For most purposes, the `fillna()` method is the workhorse function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:00:22.401574Z",
     "start_time": "2020-12-31T12:00:22.392078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:05:53.982635Z",
     "start_time": "2020-12-31T12:05:53.972635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df.fillna(value=999) # Fill missing values with a constant\n",
    "\n",
    "missing_df.fillna(missing_df.mean()) # Fill missing values with a mean\n",
    "\n",
    "missing_df.fillna(missing_df.mode().iloc[0]) # Fill missing values with a mode\n",
    "\n",
    "missing_df.fillna({\"a\": 999, \"b\": -1, \"c\": 100}) # Fill missing values by different fill value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:06:09.254719Z",
     "start_time": "2020-12-31T12:06:09.249151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:10:09.386030Z",
     "start_time": "2020-12-31T12:10:09.377504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df.fillna(method=\"ffill\") # Forward Fill - use last valid observation for filling\n",
    "\n",
    "missing_df.fillna(method=\"backfill\") # Forward fill - use next valid observation to fill gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Removing Duplicate Values\n",
    "\n",
    "---\n",
    "\n",
    "We may have duplicate values in our data due to several reasons and they can cause some difficulties during data analysis procedure. We have to identify them and then handle them properly. In other words, we have to find unique identifier for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T09:43:48.955234Z",
     "start_time": "2021-01-01T09:43:48.936416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes = pd.read_csv(\"data/athletes.csv\")\n",
    "\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:41:30.514897Z",
     "start_time": "2020-12-31T12:41:30.504372Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes.duplicated() # Boolean series indicating duplicated rows. Uses all columns to find duplicates\n",
    "\n",
    "athletes.duplicated(subset=[\"id\", \"sex\"]) # Uses only two columns for duplicate identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:42:14.538244Z",
     "start_time": "2020-12-31T12:42:14.524027Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes[athletes.duplicated()] # Shows which rows are duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T12:44:55.398233Z",
     "start_time": "2020-12-31T12:44:55.377968Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes.drop_duplicates() # Removes all duplicate rows\n",
    "\n",
    "\n",
    "athletes.drop_duplicates(subset=[\"id\", \"nationality\"]) # Remove duplicates only considering some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Replacing Values\n",
    "\n",
    "---\n",
    "\n",
    "There are situations when we just need to replace values in a Pandas Series or DataFrame. For that reason, we can use `replace()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:05:40.228313Z",
     "start_time": "2020-12-31T13:05:40.220149Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_df.replace(to_replace=np.nan, value=999) # Replace all NaN's with 999\n",
    "\n",
    "missing_df.replace(to_replace=[1.0, 3.0], value=[2.0, 4.0]) # Replace several values\n",
    "\n",
    "missing_df.replace(to_replace=[1.0, 6.5], value=np.nan) # Replace multiple values at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Discretization and Binning\n",
    "\n",
    "---\n",
    "\n",
    "Continuous data is often discretized or otherwise separated into `bins` for analysis. Suppose you have data about a group of people in a study, and you want to group them into discrete buckets.\n",
    "\n",
    "Let discretize `weight` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:25:24.177364Z",
     "start_time": "2020-12-31T13:25:24.166769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:10:29.477015Z",
     "start_time": "2020-12-31T13:10:29.454980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes[\"weight\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before we discretize weight column, let calculate `BMI - Body Mass Index` and then discretize weight according to that values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:25:32.487138Z",
     "start_time": "2020-12-31T13:25:32.477272Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes[\"bmi\"] = athletes[\"weight\"] / (athletes[\"height\"] ** 2)\n",
    "\n",
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:31:57.407137Z",
     "start_time": "2020-12-31T13:31:57.402777Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 18.5, 25, 30, 60]\n",
    "\n",
    "names = [\"underweight\", \"normal_weight\", \"overweight\", \"obese\"]\n",
    "\n",
    "athletes[\"new_weight\"] = pd.cut(athletes[\"bmi\"], bins=bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:32:01.552419Z",
     "start_time": "2020-12-31T13:32:01.542646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:34:50.099894Z",
     "start_time": "2020-12-31T13:34:50.083223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes[\"new_weight\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we pass an integer number of bins to cut instead of explicit bin edges, Pandas `cut()` will compute equal-length bins based on the minimum and maximum values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:38:14.861410Z",
     "start_time": "2020-12-31T13:38:14.849386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.cut(athletes[\"bmi\"], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Considering the distribution of the data, `cut()` method may not return equal-sized bins, while `qcut()` method by definition return approximately equal-size bins as it bins the data based on sample quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T13:46:49.707204Z",
     "start_time": "2020-12-31T13:46:49.696432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.qcut(athletes[\"bmi\"], q=10) # Deciles\n",
    "\n",
    "pd.qcut(athletes[\"bmi\"], q=4) # Quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Detecting Outliers\n",
    "\n",
    "---\n",
    "\n",
    "![alt text](images/iqr.png \"Title\")\n",
    "\n",
    "\n",
    "\n",
    "The naive approach to detect outliers is to use `InterQuartile Range - IQR`. We can use that approach to check if the `height` column contains some outliers. The formula for `IQR` is the following:\n",
    "\n",
    "$$\n",
    "\\text{IQR} = Q_{3} - Q_{1}\n",
    "$$\n",
    "\n",
    "where, $Q_{3}$ and $Q_{1}$ are upper and lower quartiles, respectively.\n",
    "\n",
    "\n",
    "From the above picture, we see that $99\\%$ of observations should be inside $\\{Q_{1} - 1.5 \\times \\text{IQR}; Q_{3} + 1.5 \\times \\text{IQR}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:22:14.675459Z",
     "start_time": "2020-12-31T17:22:14.670465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q1 = athletes[\"height\"].quantile(.25)\n",
    "q3 = athletes[\"height\"].quantile(.75)\n",
    "\n",
    "iqr = q3 - q1\n",
    "\n",
    "pmin = q1 - 1.5 * iqr\n",
    "pmax = q3 + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:22:15.147539Z",
     "start_time": "2020-12-31T17:22:15.126197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes[athletes[\"height\"].between(pmin, pmax)] # Values between IQR range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:22:15.943682Z",
     "start_time": "2020-12-31T17:22:15.927623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes[(athletes[\"height\"].lt(pmin)) | (athletes[\"height\"].gt(pmax))] # Values outside IQR range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## String Manipulations\n",
    "\n",
    "---\n",
    "\n",
    "Strings represent letters and other symbols surrounded by quotation marks. Pandas has support of string manipulation and the methods are accessible by `.str` attribute. Strings are represented as `object` data type in Pandas, instead of conventional `str`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Reference\n",
    "\n",
    "\n",
    "[Working with text data](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:27:49.557940Z",
     "start_time": "2020-12-31T17:27:49.546874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/titanic.csv\")\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:29:48.246832Z",
     "start_time": "2020-12-31T17:29:48.238204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.split(\".\") # Split string on a specified character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:32:52.964715Z",
     "start_time": "2020-12-31T17:32:52.955049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.split(\".\", expand=True) # Split string on a specified character and return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:30:46.334134Z",
     "start_time": "2020-12-31T17:30:46.329310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.strip() # Remove leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:32:19.063082Z",
     "start_time": "2020-12-31T17:32:19.058311Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.contains(\"Mrs\") # True if sub-string is included in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:34:29.888239Z",
     "start_time": "2020-12-31T17:34:29.880765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.replace(\"Mrs\", \"###\") # Replace string with other value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:37:41.667025Z",
     "start_time": "2020-12-31T17:37:41.648462Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.startswith(\"Mrs\") # True if string starts with \"Mrs\"\n",
    "\n",
    "titanic[\"Name\"].str.endswith(\"a\") # True if string ends with \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:38:32.787088Z",
     "start_time": "2020-12-31T17:38:32.780252Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Name\"].str.lower() # Lower case letter\n",
    "\n",
    "\n",
    "titanic[\"Name\"].str.upper() # Upper case letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Variable Transformation\n",
    "\n",
    "---\n",
    "\n",
    "To do a modeling, at the first stage, one have to take into consideration that we have some fixed set of models and we have to fit the data to our model, and the second is that these models have their assumptions - **which rarely holds in real world**.\n",
    "\n",
    "\n",
    "When the assumptions do not hold, we apply different transformations to our data, in order to have as desirable data format for the model as possible. By doing so, we try to extract as much information from our data as possible.\n",
    "\n",
    "\n",
    "> **The type of variable transformation greatly depends on the type of model we plan to use for modeling.**\n",
    "\n",
    "\n",
    "Transformation methods are classified in two broad class:\n",
    "\n",
    "\n",
    "* **Numeric Variable Transformation** - is turning a numeric variable to another numeric variable. Typically it is meant to change the scale of values and/or to adjust the skewed data distribution to Gaussian-like distribution through some `monotonic transformation`\n",
    "\n",
    "\n",
    "* **Categorical Variable Transformation** - is turning a categorical variable to a numeric variable. Categorical variable transformation is mandatory for most of the machine learning models because they can handle only numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Numerical Variable Transformations\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* **Standardization**\n",
    "\n",
    "\n",
    "* **Min-max scaling**\n",
    "\n",
    "\n",
    "* **Logarithmic transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:47:58.334452Z",
     "start_time": "2020-12-31T17:47:58.308655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Standardization happens using the following formula:\n",
    "\n",
    "$$\n",
    "X'_{i} = \\frac{X_{i} - \\bar{X_{n}}}{s}\n",
    "$$\n",
    "\n",
    "where, $\\bar{X_{n}}$ is an arithmetic average and $s$ is standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T18:00:48.142330Z",
     "start_time": "2020-12-31T18:00:48.135731Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(athletes['weight'] - athletes[\"weight\"].mean()) / athletes[\"weight\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Min-Max Scaling happens using the following formula:\n",
    "\n",
    "$$\n",
    "X'_{i} = \\frac{X_{i} - min(X)}{max(X) - min(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T18:03:27.540014Z",
     "start_time": "2020-12-31T18:03:27.535408Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(athletes['weight'] - athletes[\"weight\"].min()) / (athletes[\"weight\"].max() - athletes[\"weight\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Logarithmic transformation happens using by natural logarithm. However, we can use logarithm with any base. Also, note that in order to have successful logarithmic transformation the data should not contain zeros or values less than zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T18:07:32.652032Z",
     "start_time": "2020-12-31T18:07:32.637710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.log(athletes[\"weight\"]) # Natural logarithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Categorical Variable Transformations\n",
    "\n",
    "---\n",
    "\n",
    "* **One-hot encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One-hot encoding is also known as dummy variable, meaning that we create indicator or binary variable containing only zeros and ones. Pandas has built in functionality for dummy variable generation. The best candidate for dummy variable is column `sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:48:23.510495Z",
     "start_time": "2020-12-31T17:48:23.486222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "athletes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T09:47:38.781694Z",
     "start_time": "2021-01-01T09:47:38.767195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(athletes[\"sex\"]) # Returns dummy variable for any categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In column `sex` we have two values, `female` and `male`. Have a look at dummy variables above. **They are same**. In the `female` column, 0 denotes male and 1 denotes female, while in `male` column everything is in opposite direction. We conclude that we need to drop one dummy variable, since we have duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T09:54:05.326062Z",
     "start_time": "2021-01-01T09:54:05.319557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(athletes[\"sex\"], drop_first=True) # Drops one dummy variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is not an exhaustive list of variable transformations. We will cover them throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "---\n",
    "\n",
    "In this lecture, we saw how to deal with missing data, duplicate data, and how to do string manipulation, and some other analytical data transformations. In the next lecture, we focus on combining and rearranging datasets in various ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
